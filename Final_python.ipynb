{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_python.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Final Python File**"
      ],
      "metadata": {
        "id": "g2uD62l-DbA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiRQxja1wqrx",
        "outputId": "5c3fcf1e-8b94-439a-eccd-10d80d45031b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.h5  Modified_SQL_Dataset.csv  sample_data  tflite_quant_model.tflite\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "iE6cUKpHDaM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.metrics import  roc_auc_score\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "import pickle\n",
        "\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ybux4BsDojt",
        "outputId": "75b54347-dde0-445d-a14b-03018ad35d06"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Dataset**"
      ],
      "metadata": {
        "id": "nwaU6EVeD1H3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"Modified_SQL_Dataset.csv\")"
      ],
      "metadata": {
        "id": "C1WkDhbuD3RW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCWFcno6EFzs",
        "outputId": "9318373f-584e-4055-86a8-cf9fa4258f6d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30919, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Train Test DataSet**"
      ],
      "metadata": {
        "id": "3LMYGfM1EMHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "IECk3QcEQ9Sb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
        "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
        "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
        "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
        "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
        "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
        "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
        "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
        "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
        "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
        "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
        "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
        "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
        "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
        "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
      ],
      "metadata": {
        "id": "xoAA1ZcdSv_R"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining all the above stundents \n",
        "from tqdm import tqdm\n",
        "preprocessed_query = []\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# tqdm is for printing the status bar\n",
        "for sentance in tqdm(data['Query'].values):\n",
        "    sentance = re.sub('[^A-Za-z0-9]+', ' ', sentance)\n",
        "    sentance = re.sub(r',', ' ', sentance)\n",
        "    #https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
        "    tokenization = nltk.word_tokenize(sentance)\n",
        "    sentance = ' '.join([lemmatizer.lemmatize(w) for w in tokenization])\n",
        "    # https://gist.github.com/sebleier/554280\n",
        "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
        "    preprocessed_query.append(sentance.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBVsCdv-RLxG",
        "outputId": "0693d395-507d-44b4-dcd5-e8207fc0af1c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30907/30907 [00:05<00:00, 6085.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(s.split()) for s in preprocessed_query])"
      ],
      "metadata": {
        "id": "V2hrg2_QS7FA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vectorizer the text samples into 2d integer tensor\n",
        "tokenizer_obj = Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(preprocessed_query)\n",
        "sequences = tokenizer_obj.texts_to_sequences(preprocessed_query)\n",
        "\n",
        "#pad_sequences\n",
        "\n",
        "word_index = tokenizer_obj.word_index\n",
        "print('found %s unique tokens.'%len(word_index))\n",
        "\n",
        "query_pad = pad_sequences(sequences,maxlen=max_length)\n",
        "label = data['Label'].values\n",
        "print(\"shape of query_pad\",query_pad.shape)\n",
        "print(\"Shape of label\",label.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTBN2hgpS8UD",
        "outputId": "31c87369-d4e7-41bc-dc29-4b0edba71ace"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 24489 unique tokens.\n",
            "shape of query_pad (30907, 522)\n",
            "Shape of label (30907,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data into train and test data\n",
        "\n",
        "validation_split=0.2\n",
        "indices = np.arange(query_pad.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "query_pad = query_pad[indices]\n",
        "label = label[indices]\n",
        "\n",
        "num_validation_samples = int(validation_split*query_pad.shape[0])\n",
        "\n",
        "X_train_pad = query_pad[:-num_validation_samples]\n",
        "y_train = label[:-num_validation_samples]\n",
        "X_test_pad = query_pad[-num_validation_samples:]\n",
        "y_test = label[-num_validation_samples:]"
      ],
      "metadata": {
        "id": "l7EmQBo4FRwQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Function 1**"
      ],
      "metadata": {
        "id": "J9b9nZtuEqrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "def final_fun_1(X):\n",
        "\n",
        "  loaded_model = load_model('model.h5')\n",
        "  y_pred = loaded_model.predict(X_test_pad)\n",
        "  return y_pred\n",
        "final_fun_1(X_test_pad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vazrBlrkFefK",
        "outputId": "fb54d8fd-92d0-49f2-e30a-f8304ea595e9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.8426219e-08],\n",
              "       [1.0245228e-01],\n",
              "       [2.1967155e-06],\n",
              "       ...,\n",
              "       [1.0245228e-01],\n",
              "       [9.9999368e-01],\n",
              "       [1.9717445e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Function 2**"
      ],
      "metadata": {
        "id": "nyx0gaIfVU2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_fun_2(X,Y):\n",
        "    Y_pred = final_fun_1(X)\n",
        "    score = roc_auc_score(Y, Y_pred)\n",
        "    return score\n",
        "final_fun_2(X_test_pad,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84k4TgbtVcho",
        "outputId": "9db142ee-746f-499e-9148-f500a4b0fc85"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9926657577602499"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('model.h5')\n",
        "score  = loaded_model.evaluate(X_test_pad,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLi2DrIlpXKg",
        "outputId": "74d12f43-d5cc-4c4e-fd02-7b3ca00688ff"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "194/194 [==============================] - 60s 302ms/step - loss: 0.0656 - accuracy: 0.9790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test loss {:.4f}, accuracy {:.2f}%\".format(score[0], score[1] * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pip6jRlvqQgs",
        "outputId": "ca9b2f10-24d0-42d7-e62b-449c2caa02fd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss 0.0656, accuracy 97.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Float model in Mb:\", os.path.getsize('tflite_model.tflite') / float(2**20))\n",
        "print(\"Quantized model in Mb:\", os.path.getsize('tflite_quant_model.tflite') / float(2**20))\n",
        "print(\"Compression ratio:\", os.path.getsize('tflite_model.tflite')/os.path.getsize('tflite_quant_model.tflite'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9INipFSkusyK",
        "outputId": "9db5709a-e59f-4b0b-84b2-ec412dcb59e7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Float model in Mb: 29.13034439086914\n",
            "Quantized model in Mb: 7.293365478515625\n",
            "Compression ratio: 3.9940881170263065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Post Training Quantization**"
      ],
      "metadata": {
        "id": "ToD8ZGUCqc_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ref : https://stackoverflow.com/questions/65601060/tensorflowlite-error-interpreter-set-tensor/65675158#65675158\n",
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = \\\n",
        "tf.lite.Interpreter(model_path=\"tflite_quant_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "# Test model on some input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "acc=0\n",
        "for i in range(len(X_test_pad)):\n",
        "    input_data = X_test_pad[i].reshape(input_shape)\n",
        "    input_data = input_data.astype(np.float32)\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "    interpreter.invoke()\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "    if(np.argmax(output_data) == np.argmax(y_test[i])):\n",
        "        acc+=1\n",
        "acc = acc/len(X_test_pad)\n",
        "print(acc*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZos2V9eqj7b",
        "outputId": "5ce91996-0972-43c8-e262-1f1bf961d35a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FdxLHBp8qVUd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}